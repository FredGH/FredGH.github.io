{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start - Load Dependencies...\n",
      "End - Load Dependencies...\n"
     ]
    }
   ],
   "source": [
    "print(\"Start - Load Dependencies...\")\n",
    "import csv\n",
    "import gzip\n",
    "import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import tempfile\n",
    "import scipy.io\n",
    "import shutil\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from IPython.display import display, Image\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from scipy import ndimage\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Config the matlotlib backend as plotting inline in IPython\n",
    "%matplotlib inline\n",
    "print(\"End - Load Dependencies...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start - Shared Functions...\n",
      "End - Shared Functions...\n"
     ]
    }
   ],
   "source": [
    "print(\"Start - Shared Functions...\")\n",
    "def create_directory(directory_name, print_debug=True):\n",
    "    if not os.path.exists(directory_name):\n",
    "        os.makedirs(directory_name)\n",
    "        if (print_debug) == True:\n",
    "            print('Created directory: ' +  directory_name)\n",
    "    else:\n",
    "        if (print_debug) == True:\n",
    "            print('Directory: ' + directory_name + ' already exists - no need to create it')        \n",
    "print(\"End - Shared Functions...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Start - Extracting 32x32 svhn images...\")\n",
    "\n",
    "\"\"\"Helpers\"\"\"\n",
    "last_percent_reported      = None\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "  \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "  slow internet connections. Reports every 1% change in download progress.\n",
    "  \"\"\"\n",
    "  global last_percent_reported\n",
    "  percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "  if last_percent_reported != percent:\n",
    "    if percent % 5 == 0:\n",
    "      sys.stdout.write(\"%s%%\" % percent)\n",
    "      sys.stdout.flush()\n",
    "    else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent\n",
    "    \n",
    "def has_expected_object_count(directory_name, expected_object_count):\n",
    "  list = os.listdir(directory_name) \n",
    "  count = len(list)\n",
    "  if expected_object_count == count :\n",
    "     print ('Expected object count and actual object count match in: ' + directory_name)\n",
    "     return True\n",
    "  print ('No match of expect oject count:' +  str(expected_object_count)  + ' and file count: ' + str(count) + ' in directory: ' + directory_name)\n",
    "  return False\n",
    "\n",
    "def maybe_download(file_name, destination_directory, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  destination_path = destination_directory + '/' + file_name\n",
    "  print (file_name + ' --> Try download')\n",
    "  if force or not os.path.exists(destination_path):\n",
    "    create_directory(destination_directory)\n",
    "    print('...Attempting to download') \n",
    "    filename, _ = urlretrieve(url + file_name, destination_path, reporthook=download_progress_hook)\n",
    "    print('\\n...Download Complete')\n",
    "  else:\n",
    "    print('...Already exists - no need to download!')    \n",
    "  statinfo = os.stat(destination_path)  \n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('...Found and verified')\n",
    "  else:\n",
    "    raise Exception(destination_path + ' does not contain the expected files, please try the force download option.')\n",
    "  return destination_path\n",
    "\n",
    "def unpack_mat_images(source_file_name, source_directory, destination_directory, expected_object_count, force=False):\n",
    "    np.set_printoptions(threshold=np.nan)\n",
    "    source_path = source_directory + '/' + source_file_name\n",
    "    mat = scipy.io.loadmat(source_path)\n",
    "    arr_x =  np.array(mat[\"X\"]) #the image\n",
    "    arr_y =  np.array(mat[\"y\"]) #the label\n",
    "    #create directory if necessary\n",
    "    if not os.path.exists(destination_directory):\n",
    "        os.makedirs(destination_directory)\n",
    "    #if the images are already in directory, then do load them again\n",
    "    if (has_expected_object_count(destination_directory,expected_object_count) == False or force == True):\n",
    "        print('Start saving images')\n",
    "        #save each image to the destination directory\n",
    "        for i in range(expected_object_count):\n",
    "            img =  Image.fromarray(arr_x[:,:,:,i], 'RGB')\n",
    "            file_name_full_path = destination_directory + '/' + str(i+1) + '.jpg'\n",
    "            img.save(file_name_full_path)\n",
    "            print ('Saving: ' + file_name_full_path)\n",
    "        print('End saving images')\n",
    "    else:\n",
    "        print('')\n",
    "        print('Expected Images are already present in the directory: ' + destination_directory + ', no need to unpack them')           \n",
    "        \n",
    "\"\"\"Share variables\"\"\"\n",
    "url                 = 'http://ufldl.stanford.edu/housenumbers/'\n",
    "main_directory      = 'svhn_data'\n",
    "train_32_directory  = '32_train_img'\n",
    "test_32_directory   = '32_test_img'\n",
    "extra_32_directory  = '32_extra_img'\n",
    "train_32x32         = 'train_32x32.mat'\n",
    "test_32x32          = 'test_32x32.mat'\n",
    "extra_32x32         = 'extra_32x32.mat'\n",
    "\n",
    "train_32_directory = main_directory + '/' + train_32_directory\n",
    "test_32_directory = main_directory + '/' + test_32_directory\n",
    "extra_32_directory = main_directory + '/' + extra_32_directory\n",
    "\n",
    "\"\"\"Extract 32x32 svhn images\"\"\"\n",
    "print  ('Start try downloading svhn 32_32.mat files')\n",
    "train_filename = maybe_download(train_32x32,main_directory,  182040794)\n",
    "test_filename = maybe_download(test_32x32,main_directory,    64275384)\n",
    "#extra_filename = maybe_download(extra_32x32,main_directory,1329278602)\n",
    "print  ('End try downloading svhn 32_32.mat files')\n",
    "print  ('')\n",
    "print  ('Start try extracting svhn 32_32.mat images files')\n",
    "unpack_mat_images(train_32x32,main_directory,train_32_directory, 73257)\n",
    "unpack_mat_images(test_32x32,main_directory,test_32_directory,   26032)\n",
    "#unpack_mat_images(extra_32x32,main_directory,extra_32_destination_directory,531131)\n",
    "print  ('End try extracting svhn 32_32.mat images files')\n",
    "\n",
    "print(\"End - Extracting 32x32 svhn images...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Start - Extracting original svhn images...\")\n",
    "\n",
    "\"\"\"Helpers\"\"\"\n",
    "def move_file_to_directory(file_name, directory_old, directory_new):\n",
    "    create_directory(directory_new)\n",
    "    shutil.move(directory_old+'/'+file_name, directory_new+'/'+file_name)\n",
    "    print('Moved File: ' + file_name + 'from: ' + directory_old + ' to: ' + directory_new)         \n",
    "    \n",
    "def rename_directory(directory_old_name, directory_new_name):\n",
    "    if os.path.exists(directory_old_name):\n",
    "        os.rename(directory_old_name,directory_new_name)\n",
    "        print('Renamed directory: ' +  directory_old_name + ' to: ' + directory_new_name)\n",
    "\n",
    "def unpack_tar_gz_images(source_file_name, source_directory,destination_directory, root_directory, expected_object_count, force=False):\n",
    "  if (has_expected_object_count(destination_directory,expected_object_count) == False or force == True):\n",
    "    source_full_path = source_directory + '/' + source_file_name\n",
    "    print('Extracting data for ' +  source_full_path + '. This may take a while. Please wait.')\n",
    "    tar = tarfile.open(source_full_path)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall(root_directory)\n",
    "    tar.close()\n",
    "  else:\n",
    "    print('')\n",
    "    print('Expected Images are already present in the directory: ' + destination_directory + ', no need to unpack them') \n",
    "            \n",
    "\"\"\"Shared Variables\"\"\"\n",
    "url                        = 'http://ufldl.stanford.edu/housenumbers/'\n",
    "main_directory             = 'svhn_data'\n",
    "train_raw_img_directory    = 'raw_train_img'\n",
    "train_raw_mat_directory    = 'raw_train_mat'\n",
    "test_raw_img_directory     = 'raw_test_img'\n",
    "test_raw_mat_directory     = 'raw_test_mat'\n",
    "#extra_raw_img_directory    = 'raw_extra_img'\n",
    "#extra_raw_mat_directory    = 'raw_extra_mat'\n",
    "train                      = 'train.tar.gz'\n",
    "test                       = 'test.tar.gz'\n",
    "#extra                      = 'extra.tar.gz'\n",
    "digitStruct                = 'digitStruct.mat'\n",
    "see_bboxes                 = 'see_bboxes.m'\n",
    "\n",
    "train_raw_img_directory = main_directory + '/' + train_raw_img_directory\n",
    "train_raw_mat_directory = main_directory + '/' + train_raw_mat_directory\n",
    "test_raw_img_directory = main_directory + '/' + test_raw_img_directory\n",
    "test_raw_mat_directory = main_directory + '/' + test_raw_mat_directory\n",
    "#extra_raw_img_directory = main_directory + '/' + extra_raw_img_directory\n",
    "#extra_raw_mat_directory = main_directory + '/' + extra_raw_mat_directory\n",
    "\n",
    "\n",
    "\"\"\"Extracting original svhn images\"\"\"\n",
    "print  ('Start try downloading svhn .tar.gz files')\n",
    "train_filename = maybe_download(train,main_directory,  404141560)\n",
    "test_filename = maybe_download(test,main_directory,    276555967)\n",
    "#extra_filename = maybe_download(extra,main_directory,)\n",
    "print  ('End try downloading svhn .tar.gz files')\n",
    "\n",
    "print  ('')\n",
    "print  ('Start try extracting svhn .tar.gz images files')\n",
    "#create directories\n",
    "create_directory(train_raw_img_directory)\n",
    "create_directory(train_raw_mat_directory)\n",
    "create_directory(test_raw_img_directory)\n",
    "create_directory(test_raw_mat_directory)\n",
    "#first unpzip the train data, then rename the unzipped directory and move some files out\n",
    "unpack_tar_gz_images(train,main_directory,train_raw_img_directory,main_directory, 33403) #33404-1 as the mat file was removed\n",
    "rename_directory(main_directory+'/'+'train',train_raw_img_directory)\n",
    "if (has_expected_object_count(train_raw_mat_directory, 1) == False):\n",
    "    move_file_to_directory(digitStruct, train_raw_img_directory, train_raw_mat_directory)\n",
    "else:\n",
    "   print ('No file move required in: ' + train_raw_mat_directory)\n",
    "#first unpzip the test data, then rename the unzipped directory and move some files out\n",
    "unpack_tar_gz_images(test,main_directory,test_raw_img_directory,main_directory,   13069) #-13070-1 as the mat file was removed\n",
    "rename_directory(main_directory+'/'+'test',test_raw_img_directory)\n",
    "if (has_expected_object_count(test_raw_mat_directory, 1) == False):\n",
    "    move_file_to_directory(digitStruct, test_raw_img_directory, test_raw_mat_directory)\n",
    "else:\n",
    "   print ('No file move required in: ' + test_raw_mat_directory)\n",
    "#unpack_mat_images(extra,main_directory,main_directory,0)\n",
    "#rename_directory('svhn_data/extra',extra_raw_directory)\n",
    "print  ('End try extracting svhn .tar.gz images files')\n",
    "\n",
    "print  ('')\n",
    "print('Start deleting unecessary files')\n",
    "try:\n",
    "    main_directory             = 'svhn_data'\n",
    "    for directory_name in os.walk(main_directory).next()[1]:\n",
    "        directory_path = main_directory + '/' + directory_name\n",
    "        for filename in os.listdir(directory_path):\n",
    "            file_name_to_remove = 'see_bboxes.m'\n",
    "            if (file_name_to_remove in filename):\n",
    "                file_path_to_remove =  directory_path + '/' + file_name_to_remove\n",
    "                os.remove(file_path_to_remove)\n",
    "                print(file_path_to_remove + ' has been removed')\n",
    "except Exception:\n",
    "    print (\"An unexpected error occured - file deletion failed\")\n",
    "print('End deleting unecessary files')\n",
    "print(\"End Extracting original svhn images...\")\n",
    "      \n",
    "print(\"End - Extracting original svhn images...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Start Data Prepocessing Validation...\")\n",
    "\n",
    "\"\"\"Set file Path\"\"\"\n",
    "main_directory             = 'svhn_data'\n",
    "train_raw_img_directory    = 'raw_train_img'\n",
    "train_raw_csv_directory    = 'raw_train_csv'\n",
    "train_csv                  = 'train.csv'\n",
    "\n",
    "train_raw_img_directory = main_directory + '/' + train_raw_img_directory\n",
    "train_raw_csv_directory = main_directory + '/' + train_raw_csv_directory\n",
    "\n",
    "\"\"\"Check for csv incorrect data abnormalities Helpers\"\"\"\n",
    "def isBlank (myString):\n",
    "    if myString and myString.strip():\n",
    "        #myString is not None AND myString is not empty or blank\n",
    "        return False\n",
    "    #myString is None OR myString is empty or blank\n",
    "    return True\n",
    "\n",
    "\"\"\"Check for csv incorrect data abnormalities\"\"\"\n",
    "try:\n",
    "    print(\"Start checking for incorrect data in the csv file\")\n",
    "    with open(train_raw_csv_directory + '/'+ train_csv, 'rb') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "        #skip the header\n",
    "        spamreader.next()\n",
    "        for row in spamreader:\n",
    "            for i in range(len(row)):\n",
    "                if (i== 0 and isBlank(row[i])):\n",
    "                    print(\"Row:\" +  str(row) + \"is showing a empty string in the first column\")\n",
    "                if (i> 0):\n",
    "                    # if data is not castable to int (and implicity is empty or null) => then en exception is raised \n",
    "                    data = type(int(row[i])) \n",
    "                    if (i> 4):\n",
    "                        data = type(int(row[i]))\n",
    "                        if (data == 0):\n",
    "                            print(\"Row:\" +  str(row) + \"is showing a width and/or height set to 0. This is not correct\")\n",
    "                    \n",
    "except ValueError as vae:\n",
    "    print(\"This value contained in - col:\" + i + \" is not an integer\", vae)\n",
    "except Exception as e:\n",
    "    print(\"An unexpected error occured\", e)\n",
    "print(\"End checking for incorrect data in the csv file\")\n",
    "\n",
    "\"\"\"Check for image abnormalities\"\"\"\n",
    "try:\n",
    "    print(\"Start for image abnormalities\")\n",
    "    images = os.listdir(train_raw_img_directory) \n",
    "    for index in range(len(images)):\n",
    "        im=Image.open(train_raw_img_directory + '/' + images[index])\n",
    "except IOError as ioe:\n",
    "    print (\"This is not a valid image: \" + train_raw_img_directory + '/' + images[index], ioe)\n",
    "except Exception as ex:\n",
    "    print (\"An unexpected error occured: \" + train_raw_img_directory + '/' + images[index], ex)\n",
    "print(\"End for image abnormalities\")\n",
    "print(\"End Data Prepocessing Validation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Start - Data Vizualisation...\")\n",
    "\n",
    "\"\"\"Helpers\"\"\"\n",
    "def show_histogram(data_frame, x_axis_name, x_axis_label, histogram_color_name):\n",
    "    plt.hist(data_frame[x_axis_name], data_frame[x_axis_name].max(), facecolor=histogram_color_name)\n",
    "    plt.title(x_axis_name + ' Histogram')\n",
    "    plt.xlabel(x_axis_name + ' ' + x_axis_label +  ' (in pixels)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "    \n",
    "def show_box_plot(data_frame, x_axis_name):\n",
    "    plt.boxplot(data_frame[x_axis_name])\n",
    "    plt.title(x_axis_name + ' Box Plot')\n",
    "    plt.xlabel(x_axis_name + '')\n",
    "    plt.ylabel(' Size (in pixels)')\n",
    "    plt.show()\n",
    "    \n",
    "def get_statistics(data_frame,  statics_col_name, ):\n",
    "    print(data_frame[statics_col_name].describe())\n",
    "    left_median    = data_frame[statics_col_name].median()\n",
    "    print ('median     ' + str(left_median))\n",
    "        \n",
    "\"\"\"Data Analysis\"\"\"\n",
    "#view http://machinelearningmastery.com/quick-and-dirty-data-analysis-with-pandas/\n",
    "main_directory             = 'svhn_data'\n",
    "train_raw_img_directory    = 'raw_train_csv'\n",
    "\n",
    "train_raw_img_directory = main_directory + '/' + train_raw_img_directory\n",
    "\n",
    "df = pd.read_csv(train_raw_img_directory + \"/\" + \"train.csv\")\n",
    "\n",
    "#display graphs and staistics\n",
    "show_histogram(df,'Left', 'Position', 'green')\n",
    "show_box_plot(df,'Left')\n",
    "get_statistics(df,'Left')\n",
    "\n",
    "show_histogram(df,'Top', 'Position',  'red')\n",
    "show_box_plot(df,'Top')\n",
    "get_statistics(df, 'Top')\n",
    "\n",
    "show_histogram(df,'Height', 'Size', 'blue')\n",
    "show_box_plot(df,'Height')\n",
    "get_statistics(df,'Height')\n",
    "\n",
    "show_histogram(df,'Width', 'Size', 'purple')\n",
    "show_box_plot(df,'Width')\n",
    "get_statistics(df,'Width')\n",
    "\n",
    "print(\"\")\n",
    "print(\"End - Data Vizualisation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Start - Loading Massaged Trained and Test data ...\")\n",
    "\n",
    "\"\"\"Helpers\"\"\"\n",
    "def remove_file(directory_path, file_name):\n",
    "    file_full_path = directory_path + \"/\" + file_name\n",
    "    if (os.path.isfile(file_full_path) == True):\n",
    "        os.remove(file_full_path)            \n",
    "\n",
    "def rearrange(x):\n",
    "    n = x.shape[-1]\n",
    "    out = np.zeros((n, 32, 32, 3), dtype=np.float32)\n",
    "    for i in xrange(n):\n",
    "        for j in xrange(3):\n",
    "            out[i, :, :, j] = x[:, :, j, i]\n",
    "    return out / 255\n",
    "\n",
    "def flatten_keeping_order(list_of_lists):\n",
    "    flattened  = np.asarray([val for sublist in list_of_lists for val in sublist]) \n",
    "    return flattened  \n",
    "\n",
    "def hot_encode_labels(labels):\n",
    "    n = len(labels)\n",
    "    one_hot_labels = np.zeros((n, 10))\n",
    "    for i in xrange(n):\n",
    "        one_hot_labels[i, labels[i]] = 1\n",
    "    return one_hot_labels.astype(np.float32)\n",
    "\n",
    "def get_train_features_and_one_hot_labels(train_file_full_path):\n",
    "    train = loadmat(train_file_full_path) #73257\n",
    "    x_train = rearrange(train['X'])\n",
    "    y_train_original = train['y']\n",
    "    #Ytrain = train['y'].flatten() - 1\n",
    "    y_train = flatten_keeping_order(train['y']) -1\n",
    "    del train #remove ref to train (not the content)\n",
    "    #Xtrain, Ytrain = shuffle(Xtrain, Ytrain)\n",
    "    y_train_one_hot = hot_encode_labels(y_train)\n",
    "    return x_train,y_train_original, y_train, y_train_one_hot \n",
    "\n",
    "def get_test_features_and_one_hot_labels(test_file_full_path):\n",
    "    test = loadmat(test_file_full_path) \n",
    "    x_test  = rearrange(test['X'])\n",
    "    #Ytest  = test['y'].flatten() - 1\n",
    "    y_test  = flatten_keeping_order(test['y']) -1\n",
    "    del test  #remove ref to test (not the content)\n",
    "    y_test_one_hot  = hot_encode_labels(y_test)\n",
    "    return x_test,y_test,y_test_one_hot \n",
    "\n",
    "\"\"\"Set Parameters\"\"\"\n",
    "print ('CSV Set-up and Clean-up...')\n",
    "print ('\\t --> CSV files path set-up')\n",
    "run_id = 'M5000_3CL_itr100_train_73257_test_26032_2CCNLayers_3'\n",
    "print ('\\t\\t run_id:' + str(run_id))\n",
    "params_settings_file_name = 'params_settings_' + str(run_id)\n",
    "print ('\\t\\t params_settings:' + str(params_settings_file_name))\n",
    "results_file_name = 'results_' + str(run_id)\n",
    "print ('\\t\\t results_:' + str(results_file_name))\n",
    "timings_file_name = 'timings_' + str(run_id)\n",
    "print ('\\t\\t timings_:' + str(timings_file_name))\n",
    "results_directory = 'svhn_data/results'\n",
    "print ('\\t\\t results_directory:' + str(results_directory))\n",
    "\n",
    "print ('\\t --> Results Directory Clean-up')\n",
    "remove_file(results_directory, params_settings_file_name )\n",
    "remove_file(results_directory, results_file_name )\n",
    "remove_file(results_directory, timings_file_name )\n",
    "\n",
    "print ('Get the features and one hot labels...')\n",
    "x_train,y_train_original, y_train, y_train_one_hot = get_train_features_and_one_hot_labels('svhn_data/train_32x32.mat')\n",
    "x_test,y_test,y_test_one_hot = get_test_features_and_one_hot_labels('svhn_data/test_32x32.mat')\n",
    "\n",
    "print(\"End - Loading Massaged Trained and Test data ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Start - Setting Model Parameters ...\")\n",
    "\n",
    "\"\"\"Helpers\"\"\"\n",
    "def create_empty_file(directory_path, file_name):\n",
    "    create_directory(directory_path, False)\n",
    "    file_full_path = directory_path + \"/\" + file_name\n",
    "    if (os.path.isfile(file_full_path) == False):\n",
    "        os.mknod(file_full_path)\n",
    "    return file_full_path \n",
    "\n",
    "def print_and_store(directory_path,file_name, key, value):\n",
    "    print ('\\t\\t ' + key + ':' + str(value))\n",
    "    file_full_path = directory_path + \"/\" + file_name\n",
    "    cols = ('Key', 'Value')\n",
    "    if (os.path.isfile(file_full_path) == True):\n",
    "        cols = ()\n",
    "    write_to_csv(directory_path, file_name, cols, (str(key),str(value)), 'a')\n",
    "\n",
    "def write_to_csv(directory_path, file_name, header_row, data_row, write_flag):    \n",
    "    # write_flag = 'a' append to csv\n",
    "    # write_flag = 'wt' replace and write\n",
    "    file_full_path = create_empty_file(directory_path, file_name)\n",
    "    f = open(file_full_path, write_flag)\n",
    "    try:\n",
    "        writer = csv.writer(f)\n",
    "        if len(header_row)>0:\n",
    "            writer.writerow(header_row)\n",
    "        if len(data_row)>0:\n",
    "            writer.writerow( data_row )\n",
    "    finally:\n",
    "        f.close()\n",
    "\n",
    "\"\"\"Set Model Parameters \"\"\"    \n",
    "print ('\\t --> Debug')\n",
    "full_debug_info = False\n",
    "print_and_store(results_directory, params_settings_file_name,'full_debug_info', full_debug_info)\n",
    "\n",
    "print ('\\t --> Train and test sizes params')\n",
    "train_max_size = 73257 #36628\n",
    "print_and_store(results_directory, params_settings_file_name, 'train_max_size', train_max_size)\n",
    "test_max_size = 26032 #13016\n",
    "print_and_store(results_directory, params_settings_file_name, 'test_max_size', test_max_size)\n",
    "\n",
    "print ('\\t --> Gradient descent params')\n",
    "max_iter = 100\n",
    "print_and_store(results_directory, params_settings_file_name, 'max_iter', max_iter)\n",
    "batch_sz = 500 #train_max_size / 10 # for train_max_size = 1000 => batch_sz = 500\n",
    "print_and_store(results_directory, params_settings_file_name, 'batch_sz', batch_sz)\n",
    "test_batch_sz = 1000 #train_max_size / 10 # for train_max_size = 1000 => batch_sz = 500\n",
    "print_and_store(results_directory, params_settings_file_name, 'test_batch_sz', test_batch_sz)\n",
    "print_period = 10\n",
    "print_and_store(results_directory, params_settings_file_name, 'print_period', print_period)\n",
    "\n",
    "print ('\\t --> Initial weights params')\n",
    "m = 5000 # 500\n",
    "print_and_store(results_directory, params_settings_file_name, 'M', m)\n",
    "poolsz = (2, 2)\n",
    "print_and_store(results_directory, params_settings_file_name, 'poolsz', poolsz)\n",
    "\n",
    "print ('\\t --> Conv params')\n",
    "conv_strides = [1, 1, 1, 1]\n",
    "print_and_store(results_directory, params_settings_file_name, 'conv_strides', conv_strides)\n",
    "conv_padding = 'SAME'\n",
    "print_and_store(results_directory, params_settings_file_name, 'conv_padding', conv_padding)\n",
    "\n",
    "print ('\\t --> Pool params')\n",
    "pool_kernel_size = [1, 2, 2, 1]  \n",
    "print_and_store(results_directory, params_settings_file_name, 'pool_kernel_size', pool_kernel_size)\n",
    "pool_strides = [1, 2, 2, 1]\n",
    "print_and_store(results_directory, params_settings_file_name, 'pool_strides', pool_strides)\n",
    "pool_padding = 'SAME'\n",
    "print_and_store(results_directory, params_settings_file_name, 'pool_padding', pool_padding)\n",
    "\n",
    "print ('\\t --> Conv path size, channels and feature maps params')\n",
    "conv1_filter_width = 5\n",
    "print_and_store(results_directory, params_settings_file_name, 'conv1_filter_width', conv1_filter_width)\n",
    "conv1_filter_height = 5\n",
    "print_and_store(results_directory, params_settings_file_name, 'conv1_filter_height', conv1_filter_height)\n",
    "conv1_num_color_channels = 3\n",
    "print_and_store(results_directory, params_settings_file_name, 'conv1_num_color_channels', conv1_num_color_channels)\n",
    "conv1_num_feature_maps = 32\n",
    "print_and_store(results_directory, params_settings_file_name, 'conv1_num_feature_maps', conv1_num_feature_maps)\n",
    "\n",
    "conv2_filter_width = 5\n",
    "print_and_store(results_directory, params_settings_file_name, 'conv2_filter_width', conv2_filter_width)\n",
    "conv2_filter_height = 5\n",
    "print_and_store(results_directory, params_settings_file_name, 'conv2_filter_height', conv2_filter_height)\n",
    "conv2_num_color_channels = conv1_num_feature_maps\n",
    "print_and_store(results_directory, params_settings_file_name, 'conv2_num_color_channels',conv2_num_color_channels)\n",
    "conv2_num_feature_maps = 64\n",
    "print_and_store(results_directory, params_settings_file_name, 'conv2_num_feature_maps',conv2_num_feature_maps)\n",
    "\n",
    "#conv22_filter_width = 5\n",
    "#print_and_store(results_directory, params_settings_file_name, 'conv22_filter_width',conv22_filter_width)\n",
    "#conv22_filter_height = 5\n",
    "#print_and_store(results_directory, params_settings_file_name, 'conv22_filter_height',conv22_filter_height)\n",
    "#conv22_num_color_channels = conv2_num_feature_maps\n",
    "#print_and_store(results_directory, params_settings_file_name, 'conv22_num_color_channels',conv22_num_color_channels)\n",
    "#conv22_num_feature_maps = 128\n",
    "#print_and_store(results_directory, params_settings_file_name, 'conv22_num_feature_maps',conv22_num_feature_maps)\n",
    "\n",
    "print ('\\t --> Dropout params')\n",
    "drop_out_rate = 0.5 #the probability that a neuron's output is kept during dropout\n",
    "print_and_store(results_directory, params_settings_file_name, 'keep_prob',drop_out_rate)\n",
    "\n",
    "print ('\\t --> RMSPropOptimizer params')\n",
    "the_learning_rate = 0.0001 #A Tensor or a floating point value. The learning rate. \n",
    "print_and_store(results_directory, params_settings_file_name, 'the_learning_rate',the_learning_rate)\n",
    "the_decay = 0.99 #Discounting factor for the history/coming gradient \n",
    "print_and_store(results_directory, params_settings_file_name, 'the_decay',the_decay)\n",
    "the_momentum = 0.9 #A scalar tensor.\n",
    "print_and_store(results_directory, params_settings_file_name, 'the_momentum',the_momentum)\n",
    "\n",
    "print(\"End - Setting Model Parameters ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Start - Initialising and Defining the Model...\")\n",
    "\n",
    "\"\"\"Helpers\"\"\"\n",
    "def init_filter(shape, poolsz):\n",
    "    w = np.random.randn(*shape) / np.sqrt(np.prod(shape[:-1]) + shape[-1]*np.prod(shape[:-2] / np.prod(poolsz)))\n",
    "    return w.astype(np.float32)\n",
    "\n",
    "def conv_shape_weight_biais_init(filter_width, filter_height, num_color_channels, num_feature_maps, poolsz):\n",
    "    W_shape = (filter_width, filter_height, num_color_channels, num_feature_maps)\n",
    "    W_init = init_filter(W_shape, poolsz)\n",
    "    b_init = np.zeros(W_shape[-1], dtype=np.float32) # one bias per output feature map\n",
    "    return W_shape, W_init,b_init\n",
    "\n",
    "def init_weight_and_biais_variable(w_init, b_init):\n",
    "    w = tf.Variable(w_init.astype(np.float32))\n",
    "    b = tf.Variable(b_init.astype(np.float32))\n",
    "    return w,b\n",
    "\n",
    "def conv2d(X, W, b, conv_strides, conv_padding):\n",
    "    conv = tf.nn.conv2d(X, W, strides=conv_strides, padding=conv_padding)\n",
    "    conv = tf.nn.bias_add(conv, b)\n",
    "    return conv\n",
    "\n",
    "def relu(conv):\n",
    "    return tf.nn.relu(conv)\n",
    "\n",
    "def max_pool(conv, pool_kernel_size, pool_strides, pool_padding  ):\n",
    "    max_pool = tf.nn.max_pool(conv, ksize=pool_kernel_size, strides=pool_strides, padding=pool_padding)\n",
    "    return max_pool\n",
    "\n",
    "\"\"\"Init and Define Model\"\"\"\n",
    "print ('Parameters Validation...')\n",
    "if (test_max_size >len(y_test_one_hot)):\n",
    "    raise ValueError('test_max_size cannot be greater then the Ytest_one_hot lenght!')\n",
    "\n",
    "print ('Resize the training and test sets...')\n",
    "x_train = x_train[:train_max_size,]\n",
    "x_test = x_test[:test_max_size,]\n",
    "y_test = y_test[:test_max_size]\n",
    "y_test_one_hot = y_test_one_hot[:test_max_size,]\n",
    "y_train_one_hot = y_train_one_hot[:train_max_size,]\n",
    "one_hot_len = len(y_test_one_hot)\n",
    "one_hot_train_len = len(y_train_one_hot)\n",
    "\n",
    "print ('Init placeholders and variables...')\n",
    "k = len (y_test_one_hot.T)\n",
    "n = x_train.shape[0]\n",
    "n_batches = int(n / batch_sz)\n",
    "# conv shape, weights and biais init\n",
    "w1_shape, w1_init,b1_init = conv_shape_weight_biais_init (conv1_filter_width, conv1_filter_height, conv1_num_color_channels, conv1_num_feature_maps, poolsz)\n",
    "w2_shape, w2_init,b2_init = conv_shape_weight_biais_init (conv2_filter_width, conv2_filter_height, conv2_num_color_channels, conv2_num_feature_maps, poolsz)\n",
    "#w22_shape, w22_init,b22_init = conv_shape_weight_biais_init (conv22_filter_width, conv22_filter_height, conv22_num_color_channels, conv22_num_feature_maps, poolsz)\n",
    "# vanilla ANN weights init\n",
    "w3_init = np.random.randn(w2_shape[-1]*8*4*2, m) / np.sqrt(w2_shape[-1]*8*4*2 + m) #3Layers \n",
    "#W3_init = np.random.randn(W2_shape[-1]*8*8, M) / np.sqrt(W2_shape[-1]*8*8 + M) #2Layers\n",
    "b3_init = np.zeros(m, dtype=np.float32)\n",
    "w4_init = np.random.randn(m, k) / np.sqrt(m + k)\n",
    "b4_init = np.zeros(k, dtype=np.float32)\n",
    "# define variables and expressions\n",
    "# using None as the first shape element takes up too much RAM unfortunately\n",
    "x = tf.placeholder(tf.float32, shape=(batch_sz, 32, 32, 3), name='x')\n",
    "t = tf.placeholder(tf.float32, shape=(batch_sz, k), name='t')\n",
    "w1, b1 = init_weight_and_biais_variable (w1_init, b1_init)\n",
    "w2, b2 = init_weight_and_biais_variable (w2_init, b2_init)\n",
    "#w22, b22 = init_weight_and_biais_variable (w22_init, b22_init)\n",
    "#W23, b23 = init_weight_and_biais_variable (W23_init, b23_init)\n",
    "w3, b3 = init_weight_and_biais_variable (w3_init, b3_init)\n",
    "w4, b4 = init_weight_and_biais_variable (w4_init, b4_init)\n",
    "\n",
    "print ('Define convolution layers...')\n",
    "conv1 =  conv2d (x, w1, b1, conv_strides, conv_padding)\n",
    "conv1_relu =  relu(conv1)\n",
    "max_pool_1 = max_pool (conv1_relu, pool_kernel_size, pool_strides, pool_padding)              \n",
    "\n",
    "conv2 =  conv2d (max_pool_1, w2, b2, conv_strides, conv_padding)\n",
    "conv2_relu = relu(conv2)\n",
    "max_pool_2 = max_pool (conv2_relu, pool_kernel_size, pool_strides, pool_padding)              \n",
    "max_pool_2_shape = max_pool_2.get_shape().as_list()\n",
    "max_pool_2_reshape = tf.reshape(max_pool_2, [max_pool_2_shape[0], np.prod(max_pool_2_shape[1:])])\n",
    "\n",
    "#conv22 =  conv2d (max_pool_2, w22, b22, conv_strides, conv_padding) #conv2 \n",
    "#conv22_relu = relu(conv22) #conv2_relu\n",
    "#max_pool_22 = max_pool (conv22_relu, pool_kernel_size, pool_strides, pool_padding) #max_pool_2              \n",
    "#max_pool_22_shape = max_pool_22.get_shape().as_list()\n",
    "#max_pool_22_reshape = tf.reshape(max_pool_22, [max_pool_22_shape[0], np.prod(max_pool_22_shape[1:])])\n",
    "\n",
    "conv_final = tf.matmul(max_pool_2_reshape, w3) + b3 \n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "drop_out = tf.nn.dropout(conv_final, keep_prob)\n",
    "\n",
    "y_prediction = tf.nn.softmax(tf.matmul(drop_out, w4) + b4)\n",
    "\n",
    "print(\"End - Initialising and Defining the Model...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Start - Training and Measuring Model Accuracy...\")\n",
    "\n",
    "\"\"\"Helpers\"\"\"\n",
    "def error_rate(p, t):\n",
    "    return np.mean(p != t)\n",
    "\n",
    "def print_start_training_info (iteration_id, batch_id):\n",
    "    print (\"\\t Start Training...\")\n",
    "    print (\"\\t Iteration Id: \", iteration_id)\n",
    "    print (\"\\t Batch Id:\", batch_id)\n",
    "\n",
    "def print_sample_info (run_type,results_directory, timings_file_name, iteration_id, batch_id, start_index, err, start_time, full_debug_info):\n",
    "    err_rate = round(100.0*err,2)\n",
    "    accuracy_rate = round(100.0*(1.0-err),2)\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    start = start_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    elapsed = datetime.now() - start_time\n",
    "    if full_debug_info == True:\n",
    "        print (\"\\t ********Sample Test Run*************\")\n",
    "        print (\"\\t Run Type: \", run_type)\n",
    "        print (\"\\t Iteration Id: \", iteration_id)\n",
    "        print (\"\\t Batch Id:\", batch_id)\n",
    "        print (\"\\t Test Start Index: \", start_index)\n",
    "        print (\"\\t Error(%): \", err_rate)\n",
    "        print (\"\\t Accuracy(%):\" , accuracy_rate)\n",
    "        print (\"\\t Current time - Batch Calc:\", now)\n",
    "        print (\"\\t Elapsed time - Batch Calc:\", elapsed)\n",
    "    write_to_csv(results_directory, results_file_name, (), (run_type,iteration_id,batch_id,start_index, err_rate, accuracy_rate, start, now, elapsed ), 'a')\n",
    "\n",
    "def print_loss_sample_info (run_type,results_directory, timings_file_name, iteration_id, batch_id, loss, start_time, full_debug_info):\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    start = start_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    elapsed = datetime.now() - start_time\n",
    "    if full_debug_info == True:\n",
    "        print (\"\\t ********Sample Test Run*************\")\n",
    "        print (\"\\t Run Type: \", run_type)\n",
    "        print (\"\\t Iteration Id: \", iteration_id)\n",
    "        print (\"\\t Batch Id:\", batch_id)\n",
    "        print (\"\\t Loss: \", err_rate)\n",
    "        print (\"\\t Current time - Batch Calc:\", now)\n",
    "        print (\"\\t Elapsed time - Batch Calc:\", elapsed)\n",
    "    write_to_csv(results_directory, results_file_name, (), (run_type,iteration_id,batch_id,-1, loss, -1, start, now, elapsed ), 'a')\n",
    "\n",
    "    \n",
    "def print_full_info (run_type, results_directory, timings_file_name,iteration_id, batch_id, avg_err, start_time, full_debug_info):\n",
    "    avg_err_rate = round(100.0*avg_err,2)\n",
    "    avg_accuracy_rate = round(100.0*(1.0-avg_err),2)\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    start = start_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    elapsed = datetime.now() - start_time\n",
    "    if full_debug_info == True:\n",
    "        print (\"\\t *********Full Test Run*************\")\n",
    "        print (\"\\t Run Type: \", run_type)\n",
    "        print (\"\\t Iteration Id: \", iteration_id)\n",
    "        print (\"\\t Batch Id:\", batch_id)\n",
    "        print (\"\\t Error(%): \", avg_err_rate)\n",
    "        print (\"\\t Avg. Accuracy(%):\" , avg_accuracy_rate)\n",
    "        print (\"\\t Current time - Full Test Calc:\", now)\n",
    "        print (\"\\t Elapsed time - Full Test Calc:\", elapsed)\n",
    "        print (\"\\t ***********************************\")\n",
    "    write_to_csv(results_directory, results_file_name, (), ('Avg_' + run_type,iteration_id,batch_id,'', avg_err_rate, avg_accuracy_rate,start, now, elapsed ), 'a')\n",
    "\n",
    "def get_predictions(run_type, count, batch_sz, batch_sz_1, features, labels, labels_one_hot, predict_op_param, total_err):\n",
    "    start_index = 0\n",
    "    end_index = 0\n",
    "    one_hot_len = len(labels_one_hot)\n",
    "    while (start_index+batch_sz_1) <= one_hot_len:\n",
    "        #due to RAM limitations we need to have a fixed size input\n",
    "        #so as a result, we have this ugly total cost and prediction computation\n",
    "        #we also need to have a fixed size test iput and then we average.\n",
    "        end_index = start_index+batch_sz_1\n",
    "        features_sample = features[start_index:end_index]\n",
    "        labels_sample = labels[start_index:end_index]\n",
    "        labels_one_hot_sample = labels_one_hot[start_index:end_index,]\n",
    "        test_cost = 0\n",
    "        prediction = np.zeros(len(features_sample))\n",
    "        for kk in xrange(int(len(features_sample) / batch_sz)):\n",
    "            features_batch = features_sample[kk*batch_sz:(kk*batch_sz + batch_sz),]\n",
    "            labels_batch = labels_one_hot_sample[kk*batch_sz:(kk*batch_sz + batch_sz),]\n",
    "            prediction[kk*batch_sz:(kk*batch_sz + batch_sz)] = session.run(predict_op_param, feed_dict={x: features_batch, keep_prob: 1.0})\n",
    "        err = error_rate(prediction, labels_sample) #error calculation\n",
    "        print_sample_info(run_type, results_directory, results_file_name, i,j,start_index, err,batch_start_time, full_debug_info) #print sample test info\n",
    "        total_err = total_err + err #running total\n",
    "        count = count + 1 #increment the counter\n",
    "        start_index = start_index + batch_sz_1 #reset the start_index\n",
    "    return prediction, total_err, count, start_index\n",
    "    \n",
    "\"\"\"Training\"\"\"\n",
    "#print the timing information on training completion\n",
    "train_start_time = datetime.now()\n",
    "train_start_time_display = train_start_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print (\"\\t Start time - \" + train_start_time_display)\n",
    "write_to_csv(results_directory, timings_file_name, ('Start_Time', 'End_Time'), (train_start_time_display, ''), 'a')\n",
    "write_to_csv(results_directory, results_file_name, ('Type','Iteration_Id', 'Batch_Id','Test_Start_Index','Error_%','Accuracy_%','Start_Time', 'Current_Time','Elapsed_Time'), ( ), 'a')\n",
    "\n",
    "#define the cost function that needs minimising during training \n",
    "loss_op = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(y_prediction, t))\n",
    "train_op = tf.train.RMSPropOptimizer(learning_rate=the_learning_rate, decay=the_decay, momentum=the_momentum).minimize(loss_op)\n",
    "#define the prediction function\n",
    "predict_op = tf.argmax(y_prediction, 1)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "session = tf.Session()\n",
    "session.run(init)\n",
    "\n",
    "print(\"\\t drop_out_rate:\" + str(drop_out_rate))\n",
    "for i in xrange(max_iter):\n",
    "    avg_err = 0.0\n",
    "    avg_err_train = 0.0\n",
    "    avg_loss = 0.0\n",
    "    total_err = 0.0\n",
    "    total_err_train = 0.0\n",
    "    total_loss = 0.0\n",
    "    test_count = 0\n",
    "    train_count = 0\n",
    "    loss = 0.0\n",
    "    print (\"\\t Nb of batches: \"+ str(n_batches))\n",
    "    for j in xrange(n_batches):\n",
    "        batch_start_time = datetime.now()\n",
    "        x_batch = x_train[j*batch_sz:(j*batch_sz + batch_sz),]\n",
    "        y_batch = y_train_one_hot[j*batch_sz:(j*batch_sz + batch_sz),]\n",
    "        if len(x_batch) == batch_sz:\n",
    "            total_loss = total_loss + loss\n",
    "            res = session.run([train_op,loss_op], feed_dict={x: x_batch, t: y_batch, keep_prob: drop_out_rate})\n",
    "            loss = res[1]\n",
    "            if j % print_period == 0:\n",
    "                #print the iteration and batch id\n",
    "                print_start_training_info(i,j) \n",
    "                #store the loss info        \n",
    "                print_loss_sample_info(\"Loss\", results_directory, results_file_name, i,j,loss,batch_start_time, full_debug_info) \n",
    "                #calc train predictions and store training accuracy\n",
    "                prediction_train, total_err_tain, train_count, train_start_index = get_predictions(\"Train\", train_count, batch_sz, batch_sz, x_train, y_train, y_train_one_hot, predict_op, total_err_train)\n",
    "                #calc test predictions and store test accuracy\n",
    "                prediction, total_err, test_count, test_start_index = get_predictions(\"Test\", test_count, batch_sz, test_batch_sz, x_test, y_test, y_test_one_hot, predict_op, total_err)\n",
    "    avg_err = total_err / float(test_count) #linearly average all test errors\n",
    "    avg_err_train = total_err_train / float(train_count) #linearly average all training errors\n",
    "    avg_loss = total_loss / float(n_batches) #linearly average all loss    \n",
    "    print_full_info(\"Test\", results_directory, results_file_name,i,j, avg_err, batch_start_time, full_debug_info) \n",
    "    print_full_info(\"Train\", results_directory, results_file_name,i,j, avg_err_train, batch_start_time, full_debug_info) \n",
    "    print_full_info(\"Loss\", results_directory, results_file_name,i,j, avg_loss, batch_start_time, full_debug_info) \n",
    "\n",
    "#print the timing information on training completion\n",
    "train_end_time = datetime.now()\n",
    "train_end_time_display = train_end_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print (\"\\t End time - \" + train_end_time_display)\n",
    "write_to_csv(results_directory, timings_file_name, (), ('', train_end_time_display), 'a')\n",
    "\n",
    "print(\"End - Training and Measuring Model Accuracy...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
